{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T11:00:45.139734Z",
     "start_time": "2024-04-25T11:00:45.126731Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"../datasets/llm_train_essays.pkl\"\n",
    "CHECKPOINT_PATH = \"../model/deberta-v3-base\"\n",
    "# Model parameters\n",
    "METRIC_NAME = \"roc_auc\"\n",
    "MODEL_NAME = \"deberta-base\"\n",
    "MAX_LEN = 1024\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "LABEL_SMOOTHING = 0.05\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "\n",
    "# Import and clean dataset\n",
    "def import_dataset(dataset):\n",
    "    if dataset.endswith(\".pkl\"):\n",
    "        train = pd.read_pickle(dataset)\n",
    "    elif dataset.endswith(\".csv\"):\n",
    "        train = pd.read_csv(dataset)\n",
    "    train = train.dropna(subset=[\"text\"])\n",
    "    train = train.drop_duplicates(subset=[\"text\"])\n",
    "    train = train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train = train.sample(n=30000)\n",
    "    X = train[\"text\"]\n",
    "    if \"label\" in train.columns:\n",
    "        y = train[\"label\"]\n",
    "    else:\n",
    "        y = train[\"generated\"]\n",
    "    print(\"Dataset shape:\", train.shape)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Train and evaluate DeBERTa model\n",
    "def deberta(X_train, y_train, X_test, y_test):\n",
    "    # Load pretrained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        num_labels=2,\n",
    "        max_position_embeddings=512,\n",
    "    )\n",
    "\n",
    "    # Build train and validation Datasets\n",
    "    def preprocess_func(row):\n",
    "        return tokenizer(\n",
    "            row[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "        )\n",
    "\n",
    "    train = Dataset.from_pandas(pd.DataFrame({\"text\": X_train, \"label\": y_train}))\n",
    "    test = Dataset.from_pandas(pd.DataFrame({\"text\": X_test, \"label\": y_test}))\n",
    "    enc_cache_file = f\"{CHECKPOINT_PATH}/enc_cache_{MAX_LEN}.pkl\"\n",
    "    if os.path.exists(enc_cache_file):\n",
    "        train_enc, test_enc = pickle.load(open(enc_cache_file, \"rb\"))\n",
    "    else:\n",
    "        train_enc = train.map(preprocess_func, batched=True, remove_columns=[\"text\"])\n",
    "        test_enc = test.map(preprocess_func, batched=True, remove_columns=[\"text\"])\n",
    "        train_enc.set_format(type=\"torch\")\n",
    "        test_enc.set_format(type=\"torch\")\n",
    "        pickle.dump((train_enc, test_enc), open(enc_cache_file, \"wb\"))\n",
    "\n",
    "    # Build trainer\n",
    "    num_steps = len(X_train) * NUM_EPOCHS // (TRAIN_BATCH_SIZE * GRAD_ACCUM_STEPS)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"../models/{MODEL_NAME}-finetuned\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=num_steps // 5,\n",
    "        save_steps=num_steps // 5,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=METRIC_NAME,\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "\n",
    "        # roc auc\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "        auc = roc_auc_score(labels, probs[:, 1], multi_class=\"ovr\")\n",
    "\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        precision = precision_score(y_pred=predictions, y_true=labels)\n",
    "        recall = recall_score(y_pred=predictions, y_true=labels)\n",
    "        f1 = f1_score(y_pred=predictions, y_true=labels)\n",
    "        accuracy = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"roc_auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            \"roc_auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "\n",
    "    model = model.cuda()\n",
    "    print(f\"Model loaded on {model.device}\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_enc,\n",
    "        eval_dataset=test_enc,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer.train()\n",
    "    # logits = trainer.predict(test_enc).predictions\n",
    "    # y_pred = (np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True))[:, 1]\n",
    "    # return roc_auc_score(y_test, y_pred)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-25T11:00:45.140731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import dataset\n",
    "X, y = import_dataset(DATASET_PATH)\n",
    "\n",
    "# Split train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, stratify=y\n",
    ")\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project='Detect LLM Text')\n",
    "# Train model\n",
    "deberta(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "# Clean up\n",
    "del X, y, X_train, X_test, y_train, y_test\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "23c16fb83f793805",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
