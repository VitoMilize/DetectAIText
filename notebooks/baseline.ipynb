{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "id": "xVZYU40LyE7x",
    "ExecuteTime": {
     "end_time": "2024-04-05T18:46:48.828504Z",
     "start_time": "2024-04-05T18:46:48.810501Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv('../datasets/test_essays.csv')\n",
    "submission_df = pd.read_csv('../datasets/sample_submission.csv')\n",
    "train_df = pd.read_csv(\"../datasets/train_v2_drcat_02.csv\")"
   ],
   "metadata": {
    "id": "fNkajketyJIk",
    "ExecuteTime": {
     "end_time": "2024-04-05T18:46:49.981500Z",
     "start_time": "2024-04-05T18:46:49.002501Z"
    }
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    text  label\n0      Phones\\n\\nModern humans today are always on th...      0\n1      This essay will explain if drivers should or s...      0\n2      Driving while the use of cellular devices\\n\\nT...      0\n3      Phones & Driving\\n\\nDrivers should not be able...      0\n4      Cell Phone Operation While Driving\\n\\nThe abil...      0\n...                                                  ...    ...\n43479  There has been a fuss about the Elector Colleg...      0\n43480  Limiting car usage has many advantages. Such a...      0\n43481  There's a new trend that has been developing f...      0\n43482  As we all know cars are a big part of our soci...      0\n43483  Cars have been around since the 1800's and hav...      0\n\n[27371 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Phones\\n\\nModern humans today are always on th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This essay will explain if drivers should or s...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Driving while the use of cellular devices\\n\\nT...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43479</th>\n      <td>There has been a fuss about the Elector Colleg...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43480</th>\n      <td>Limiting car usage has many advantages. Such a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43481</th>\n      <td>There's a new trend that has been developing f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43482</th>\n      <td>As we all know cars are a big part of our soci...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43483</th>\n      <td>Cars have been around since the 1800's and hav...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>27371 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(columns=['prompt_name', 'source', 'RDizzl3_seven'])\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T18:46:49.996503Z",
     "start_time": "2024-04-05T18:46:49.982502Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         id          text\n0  0000aaaa  Aaa bbb ccc.\n1  1111bbbb  Bbb ccc ddd.\n2  2222cccc  CCC ddd eee.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>Aaa bbb ccc.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1111bbbb</td>\n      <td>Bbb ccc ddd.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2222cccc</td>\n      <td>CCC ddd eee.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.drop(columns='prompt_id')\n",
    "test_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T15:25:18.822488Z",
     "start_time": "2024-04-04T15:25:18.808489Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1. Text Preprocessing"
   ],
   "metadata": {
    "id": "_xSnuhLuyQ9Q"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df[\"generated\"] = train_df[\"label\"].apply(lambda x: 1.0 if x == 1 else 0.0)\n",
    "train_df[\"human\"] = train_df[\"label\"].apply(lambda x: 1.0 if x == 0 else 0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T06:47:39.279774Z",
     "start_time": "2024-04-01T06:47:39.248776Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2. Modeling"
   ],
   "metadata": {
    "id": "Tt0e5xkSwRJY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(train_df, test_size=0.30, random_state=42, shuffle=True, stratify=train_df[\"label\"])"
   ],
   "metadata": {
    "id": "pZVRw8Si2o4L",
    "ExecuteTime": {
     "end_time": "2024-04-01T06:47:40.031777Z",
     "start_time": "2024-04-01T06:47:39.280775Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\")\n",
    "val.to_csv(\"val.csv\")\n",
    "test_df.to_csv(\"test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T06:47:42.342281Z",
     "start_time": "2024-04-01T06:47:40.080774Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "id": "VLdplTrt38ZP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from tokenizers import (\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "LABELS = ['generated', 'human']\n",
    "id2label = {idx:label for idx, label in enumerate(LABELS)}\n",
    "label2id = {label:idx for idx, label in enumerate(LABELS)}\n",
    "\n",
    "def read_csv_with_labels(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    texts = data['text'].tolist()\n",
    "    labels = data[LABELS].values\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "def read_csv_without_labels(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    texts = data['text'].tolist()\n",
    "\n",
    "    return texts\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "            'roc_auc': roc_auc,\n",
    "            'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def create_tokenizer():\n",
    "    raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    raw_tokenizer.normalizer = normalizers.NFC()\n",
    "    raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    \n",
    "    VOCAB_SIZE = 30522\n",
    "    special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "          \n",
    "    raw_tokenizer.train_from_iterator(test_df[['text']].values, trainer=trainer)\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=raw_tokenizer,\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "    )\n",
    "\n",
    "class LLMDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(val[idx])\n",
    "            for key, val in self.encodings.items()\n",
    "        }\n",
    "\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "class LLMDTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(val[idx])\n",
    "            for key, val in self.encodings.items()\n",
    "        }\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "\n",
    "class ClassificationTrainer():\n",
    "    def __init__(self,\n",
    "                 pretrained_transformer_name='distilbert-base-cased',\n",
    "                 dataset_dct={'train':'train.csv', 'val': 'val.csv', 'test':'test.csv'},\n",
    "                 warmup_steps=500,\n",
    "                 num_train_epochs=3):\n",
    "\n",
    "        train_texts, train_labels = read_csv_with_labels(dataset_dct['train'])\n",
    "        val_texts, val_labels = read_csv_with_labels(dataset_dct['val'])\n",
    "        test_texts = read_csv_without_labels(dataset_dct['test'])\n",
    "\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(pretrained_transformer_name)\n",
    "        self.tokenizer.save_pretrained(\"/tokenizer\")\n",
    "\n",
    "        train_encodings = self.tokenizer(train_texts, truncation=True, max_length=256, padding=True)\n",
    "        val_encodings = self.tokenizer(val_texts, truncation=True, max_length=256, padding=True)\n",
    "        test_encodings = self.tokenizer(test_texts, truncation=True, max_length=256, padding=True)\n",
    "\n",
    "        self.train_dataset = LLMDDataset(train_encodings, train_labels)\n",
    "        self.val_dataset = LLMDDataset(val_encodings, val_labels)\n",
    "        self.test_dataset = LLMDTestDataset(test_encodings)\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                pretrained_transformer_name, num_labels=len(LABELS), problem_type=\"multi_label_classification\",  id2label=id2label, label2id=label2id)\n",
    "\n",
    "        self.metric = {metric:load_metric(metric) for metric in ['f1', 'precision', 'recall', 'accuracy']}\n",
    "\n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir='./results',  # output directory\n",
    "            num_train_epochs=num_train_epochs, # total number of training epochs\n",
    "            per_device_train_batch_size=\n",
    "            64,  # batch size per device during training\n",
    "            per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "            warmup_steps=\n",
    "            warmup_steps,  # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,  # strength of weight decay\n",
    "            logging_dir='./logs',  # directory for storing logs\n",
    "            logging_strategy='epoch',\n",
    "            evaluation_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            save_total_limit = 3,\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "            args=self.training_args,  # training arguments, defined above\n",
    "            train_dataset=self.train_dataset,  # training dataset\n",
    "            eval_dataset=self.val_dataset,  # evaluation dataset\n",
    "            compute_metrics=self.compute_metrics,\n",
    "        )\n",
    "\n",
    "\n",
    "    def compute_metrics(self, p: EvalPrediction):\n",
    "        predictions = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        result = multi_label_metrics(\n",
    "            predictions=predictions,\n",
    "            labels=p.label_ids\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "    def inference(self):\n",
    "        predictions = self.trainer.predict(self.test_dataset, metric_key_prefix=\"predict\").predictions\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "id": "1DnKMoEN33xH",
    "ExecuteTime": {
     "end_time": "2024-04-01T06:47:51.771282Z",
     "start_time": "2024-04-01T06:47:42.343284Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classification_trainer = ClassificationTrainer(\n",
    "    pretrained_transformer_name='cointegrated/rubert-tiny2',\n",
    "    dataset_dct={'train':'train.csv', 'val': 'val.csv', 'test': 'test.csv'},\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "classification_trainer.trainer.train()"
   ],
   "metadata": {
    "id": "NpII9xBoESF-",
    "ExecuteTime": {
     "start_time": "2024-04-01T06:47:51.772283Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gasoline stored in the fuel tank of a vehicle can escape from the vehicle and pollute the environment, even when the vehicle is not running. This occurs because gasoline is volatile and can change from liquid to gas, which can pass into the air. Evaporated gasoline escaping from fuel tanks is a significant source of environmental pollution with volatile organic compounds (VOCs), which can harm the environment and human health. To prevent this leakage of gasoline, modern vehicles are equipped with a canister packed with particles of activated carbon, which captures the gasoline molecules in a maze of carbon molecules. Activated carbon is a charcoal material widely used for the purification of drinking water and natural gas. The adsorption of evaporated gasoline on activated carbon can be compared to the Labyrinth of the Minotaur. The labyrinth passages must be cleaned so that they can adsorb new VOCs the next day. The vehicleâ€™s engine acts as the Minotaur, by feeding on the VOCs. The next time the vehicle is started, air is passed through the canister to separate VOCs from the walls of the activated carbon passages. This process is called desorption, which means unsticking the molecules that are adsorbed to the passages. These desorbed molecules are then burned in the engine. To further improve the adsorption capacity of activated carbon, a waste product called agave bagasse is processed to create a new labyrinth. The addition of carbon nanotubes to the biochar passages increases the capacity of biochar to adsorb VOCs. The production of carbon labyrinths from agave bagasse shows how a waste material can be of interest to the automotive industry and possibly other industries. The adsorption process is also being used to remove pollutants such as arsenic and fluoride from drinking water in countries where these contaminants are a problem.\n",
      "[1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\vitya\\AppData\\Local\\Temp\\ipykernel_3564\\1032734142.py:119: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.metric = {metric:load_metric(metric) for metric in ['f1', 'precision', 'recall', 'accuracy']}\n",
      "C:\\Users\\vitya\\PycharmProjects\\DetectAIText\\.venv\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vitya\\PycharmProjects\\DetectAIText\\.venv\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vitya\\PycharmProjects\\DetectAIText\\.venv\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vitya\\PycharmProjects\\DetectAIText\\.venv\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vitya\\PycharmProjects\\DetectAIText\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='62' max='17688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   62/17688 01:07 < 5:30:31, 0.89 it/s, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = classification_trainer.inference()\n",
    "\n",
    "submission_df['generated'] = predictions\n",
    "submission_df.to_csv('../submission.csv', index=False)"
   ],
   "metadata": {
    "id": "zYFgJFaHHqoW"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
